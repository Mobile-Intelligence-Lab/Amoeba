# Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning

*Official implementation of Amoeba in Pytorch*

### Environment Setup
Create a new enviroment with conda:
```shell
conda create -n amoeba python=3.9
conda activate amoeba
```
and install required pacakges by:
```shell
pip install -r requirements.txt
```

## Data Collection
The configuration files to set up V2ray client/server proxy are provided under `./data/v2ray-config`. 
It is required to configure V2ray server on a machine with a public IP and a domain name pointing to the IP address.

Similarly, we set up a Tor relay node to which the tor client connects. The configuration files for tor client/relay are under `./data/tor-config`. Note that the configuration of relay node is not necessary but a fixed IP would help identify the connections generated by Tor and simplify the script to extract features.

We also provide the scripts under `./data` to crawl and extract flow features (in accordance to the flow representation in our paper). Remember to populate relevant IP and port number.

Preprocessed Tor and V2ray datasets are available under `./data/preprocessed`.


## Training StateEncoder
As a building block of Amoeba, StateEncoder should be pre-trained on a randomly generated dataset.
Use the following command to train StateEncoder:
```shell
python3 train_stateencoder.py --ROOT <absolute path of this repo> --exp_name seq2seq
```
You can customize the following hyperparameters:
```
optional arguments:
  --ROOT ROOT           absolute directory of Amoeba
  --exp_name EXP_NAME
                        experiment name
  --batch_size BATCH_SIZE
                        batch size (default: 512)
  --hidden_size HIDDEN_SIZE
                        the hidden size of GRU (default: 256)
  --num_layers NUM_LAYERS
                        num layers (default: 2)
  --epoch_num EPOCH_NUM
                        epoch num (default: 1000)
  --state_num STATE_NUM
                        {1, 2}
                        2: encode both packet size and IAT. 
                        1: encode packet size only (default: 2)
  --max_len MAX_LEN     max flow len to encode (default: 80)
  --lr LR               learning rate (default: 0.0001)
  --device DEVICE       training device: {'cuda', 'cpu'} (default: cuda)
  --train_num TRAIN_NUM
                        size of randomly generated train set (default: 100000)
  --test_num TEST_NUM   size of randomly generated test set (default: 3000)

```
It is recommended to set `max_len` higher than the actual maximal length of flows that you would like
to encode. For example, when training StateEncoder, set `max_len` to 70. And set `max_len` in
Amoeba to 60 to reduce the potential encoding errors that may occur.
When computing the loss during training, `y_hat` is flipped, meaning that the first output from StateDecoder
corresponds to the last input to StateEncoder, and so on and so forth. Training in this way guarantees that 
the encoded states memorize better the most recent input than old ones, and makes it easier for the agent to 
understand how the overhead penalty is generated. 

## Training Censoring Classifiers and Amoeba
The following is an example command to train Decision Tree as the censoring classifier 
and then to train Amoeba to attack the classifier:
```shell
python3 train_amoeba.py --ROOT <absolute path of this repo> \
                        --exp_name amoeba_dt \
                        --data_config tor_data.json \
                        --dis dt \
                        --encoder_path <acquired from the previous step>
```
All arguments are shown below:
```
optional arguments:
  -h, --help            show this help message and exit
  --ROOT ROOT           The directory of Amoeba (default: None)
  --exp_name EXP_NAME   experiment name (default: None)
  --data_config DATA_CONFIG
                        the json file of dataset paths (default: None)
  --dis DIS             censoring classifier type, 
                        supports {'dt', 'rf', 'cumul', 'df', 'lstm', 'sdae'} (default: dt)
  --trained_dis_path TRAINED_DIS_PATH
                        path of trained censoring classifier (default: None)
  --train_epochs TRAIN_EPOCHS
                        epoch size to training censoring classifiers if 
                        a trained model is not provided (default: 10)
  --encoder_path ENCODER_PATH
                        trained StateEncoder path (default: None)
  --enc_dim ENC_DIM     StateEncoder hidden dim (default: 256)
  --layer_num LAYER_NUM
                        StateEncoder layer num (default: 2)
  --max_episode_length MAX_EPISODE_LENGTH
                        the max encoding length of StateEncoder (default: 60)
  --learning_rate LEARNING_RATE
                        learning rate (default: 0.0005)
  --data_penalty DATA_PENALTY
                        data penalty (default: 1)
  --truncate_penalty TRUNCATE_PENALTY
                        truncate penalty (default: 0.05)
  --time_penalty TIME_PENALTY
                        time penalty (default: 2)
  --MAX_UNIT MAX_UNIT   supports {1448, 16500}. 1448: tor, 16500: v2ray (default: 1448)
  --MAX_DELAY MAX_DELAY
                        in second (default: 1)
  --binary_signal BINARY_SIGNAL
                        use classification scores for training if False (default: False)
  --adv_pkt_clip ADV_PKT_CLIP
                        no clipping by default (default: 1)
  --adv_iat_clip ADV_IAT_CLIP
                        per-packet added delay is limited within 0.05 (default: 0.005)
  --timesteps TIMESTEPS
                        training steps (default: 300000)
  --test_num TEST_NUM   number of test samples (default: 3000)
  --mask_rate MASK_RATE
                        reward mask rate (default: 0)
  --action_mode ACTION_MODE
                        {`addition_only`, `direction_limited`, `direction_unlimited`} are supported. 
                        `addition_only`: allow padding only; 
                        `direction_limited`: allow padding and truncation; 
                        `direction_unlimited`: allow padding and truncation,
                        dummy packets with different directions can be generated between truncated packets. 
                        The final option would potentially introduce the largest overhead 
                        as well as highest ASR. (default: direction_limited)

```
### Training tips:
1. In some cases, Amoeba may generate adversarial flows with high time overhead, but note that time overhead
within 15% is sufficient to mislead the censoring classifiers. One can set `adv_iat_clip` < 0.05 which puts
a limit to the maximal added delay.
2. Amoeba is required to optimize multiple metrics during training, including attack success rate, data overehad and time overhead.
The current script saves checkpoints periodically while the last model saved may not produce the highest ASR.
It is recommended to track the training logs in tensorboard by `tensorboard --logdir <log dir>` to 
understand how each metric evolves during training, and final a desirable checkpoint for the final test. You can also modify the model saving strategy 
in the callback function in `src/logger.py`.
3. Amoeba provides three action modes as introduced above. "addition_only" allows padding solely during training.
This is an experimental mode that aims at testing which censoring classifiers can be circumvented by 
adding padding only. The difference between "direction_limited" and
"direction_unlimited" is whether dummy packets from a different direction
can be generated if a truncated packet has not been fully transmitted. Both modes require the senders to
negotiate who can transmit packets at each step from a protocol specification perspective, but
performance-wise, "direction_unlimited" would always produce higher ASR at the cost of higher overhead. 
The trained models in our repository are obtained by using  ``direction\_limited''.
We recommend to always start from the simplest mode and try more sophisticated one if needed.
4. `data_config` argument expects a json file that provides the paths of `clf_train`, `agent_train` and `val` and `test` sets.
The data config files for both datasets are `./data/config/tor_data.json` and `./data/config/v2ray_data.json`.

## Testing censoring classifiers and Amoeba
We also provided a batch of trained models under `./saved_models/` for testing. As an example, you may run the following 
command to test Amoeba trained against Decision Tree on Tor dataset:
```shell
python3 test_amoeba.py --ROOT <absolute path of this repo> \
                       --dis dt \
                       --dataset tor
```
The following options are supported:
```
  --ROOT ROOT        The directory of Amoeba (default: None)
  --dis DIS          censoring classifier type, supports {'dt', 'rf', 'cumul', 'df', 'lstm', 'sdae'} (default: dt)
  --dataset DATASET  dataset type, supports {'tor', 'v2ray'} (default: v2ray)
```

## Benchmark Algorithms
[BAP](https://github.com/SPIN-UMass/BLANKET) and
[C&W Attack](https://github.com/Trusted-AI/adversarial-robustness-toolbox) are publicly available. We open sourced
our implementation of NIDSGAN in `nidsgan.py`.


## Citation
<pre><code>@inproceedings{amoeba,
  author = {Liu, Haoyu and Diallo, Alec F. and Patras, Paul},
  title = {Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning},
  booktitle={Proceedings of the 13th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)},
  year={2023}
}</code></pre>